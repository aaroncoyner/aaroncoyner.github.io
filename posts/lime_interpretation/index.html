<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Aaron Coyner">
    <meta name="description" content="Aaron Coyner&#39;s Personal Website">
    <meta name="keywords" content="bioinformatics, computational biology, machine learning, personal">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Tutorial: Global and Local Model Interpretation"/>
<meta name="twitter:description" content="Data Preparation First, we’ll load the necessary libraries and the Cardiovascular Disease Risk Dataset.
## Install cvdRiskData using devtools::install_github(&quot;laderast/cvdRiskData&quot;) library(cvdRiskData) library(tidyverse) library(keras) library(lime) library(caret) library(ROCR) library(corrr) library(tidyquant) ## Set seed for repeatability set.seed(655) ## Load data from cvdRiskData package data(cvd_patient) ## Remove columns patientID, (binned) aged (i.e. 10-20, 21-30, etc.), and race cvd_patient &lt;- select(cvd_patient, -patientID, -age, -race) summary(cvd_patient) ## htn treat smoking t2d gender numAge ## N:310265 N:358070 N:374322 N:393906 F:243138 Min."/>

    <meta property="og:title" content="Tutorial: Global and Local Model Interpretation" />
<meta property="og:description" content="Data Preparation First, we’ll load the necessary libraries and the Cardiovascular Disease Risk Dataset.
## Install cvdRiskData using devtools::install_github(&quot;laderast/cvdRiskData&quot;) library(cvdRiskData) library(tidyverse) library(keras) library(lime) library(caret) library(ROCR) library(corrr) library(tidyquant) ## Set seed for repeatability set.seed(655) ## Load data from cvdRiskData package data(cvd_patient) ## Remove columns patientID, (binned) aged (i.e. 10-20, 21-30, etc.), and race cvd_patient &lt;- select(cvd_patient, -patientID, -age, -race) summary(cvd_patient) ## htn treat smoking t2d gender numAge ## N:310265 N:358070 N:374322 N:393906 F:243138 Min." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aaroncoyner.github.io/posts/lime_interpretation/" />
<meta property="article:published_time" content="2019-08-22T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-08-22T00:00:00&#43;00:00"/>


    
      <base href="https://aaroncoyner.github.io/posts/lime_interpretation/">
    
    <title>
  Tutorial: Global and Local Model Interpretation · Aaron Coyner, PhD(C)
</title>

    
      <link rel="canonical" href="https://aaroncoyner.github.io/posts/lime_interpretation/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://aaroncoyner.github.io/css/coder.min.28d751104f30c16da1aa1bb04015cbe662cacfe0d1b01af4f2240ad58580069c.css" integrity="sha256-KNdREE8wwW2hqhuwQBXL5mLKz&#43;DRsBr08iQK1YWABpw=" crossorigin="anonymous" media="screen" />
    

    

    

    
      <link rel="stylesheet" href="https://aaroncoyner.github.io/css/academicons.min.css" />
    

    
    
    <link rel="icon" type="image/png" href="https://aaroncoyner.github.io/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://aaroncoyner.github.io/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.55.6" />
  </head>

  <body class=" ">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://aaroncoyner.github.io">
      <i class="fa fa-home"></i> Aaron Coyner, PhD(C)
    </a>
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://aaroncoyner.github.io/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://aaroncoyner.github.io/posts/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://aaroncoyner.github.io/publications/">Publications</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://aaroncoyner.github.io/cv/">CV</a>
          </li>
        
      
      
    </ul>
  </section>
</nav>


<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Tutorial: Global and Local Model Interpretation</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2019-08-22T00:00:00Z'>
                August 22, 2019
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              13 minutes read
            </span>
          </div>
          
          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="https://aaroncoyner.github.io/tags/r/">R</a>
      <span class="separator">•</span>
    <a href="https://aaroncoyner.github.io/tags/statistics/">Statistics</a>
      <span class="separator">•</span>
    <a href="https://aaroncoyner.github.io/tags/interpretability/">Interpretability</a></div>

        </div>
      </header>

      <div>
        


<div id="data-preparation" class="section level2">
<h2>Data Preparation</h2>
<p>First, we’ll load the necessary libraries and the Cardiovascular Disease Risk Dataset.</p>
<pre class="r"><code>## Install cvdRiskData using devtools::install_github(&quot;laderast/cvdRiskData&quot;)
library(cvdRiskData)
library(tidyverse)
library(keras)
library(lime)
library(caret)
library(ROCR)
library(corrr)
library(tidyquant)

## Set seed for repeatability
set.seed(655)

## Load data from cvdRiskData package
data(cvd_patient)

## Remove columns patientID, (binned) aged (i.e. 10-20, 21-30, etc.), and race
cvd_patient &lt;- select(cvd_patient, -patientID, -age, -race)

summary(cvd_patient)</code></pre>
<pre><code>##  htn        treat      smoking    t2d        gender         numAge     
##  N:310265   N:358070   N:374322   N:393906   F:243138   Min.   : 0.00  
##  Y:114930   Y: 67125   Y: 50873   Y: 31289   M:182057   1st Qu.:29.00  
##                                                         Median :44.00  
##                                                         Mean   :44.01  
##                                                         3rd Qu.:59.00  
##                                                         Max.   :90.00  
##       bmi            tchol            sbp        cvd       
##  Min.   :15.00   Min.   :155.0   Min.   : 73.0   N:375325  
##  1st Qu.:19.00   1st Qu.:160.0   1st Qu.:115.0   Y: 49870  
##  Median :21.00   Median :180.0   Median :124.0             
##  Mean   :21.98   Mean   :187.7   Mean   :135.5             
##  3rd Qu.:24.00   3rd Qu.:206.0   3rd Qu.:165.0             
##  Max.   :36.00   Max.   :245.0   Max.   :222.0</code></pre>
<p>We see that we have 9 predictors:</p>
<ul>
<li><code>htn</code>: patient has (1) or does not have (0) hypertension</li>
<li><code>treat</code>: patient is (1) or is not (0) receiving treatment for hypertension</li>
<li><code>smoking</code>: patient is (1) or is not (0) a smoker</li>
<li><code>t2d</code>: patient has (1) or does not have (0) type II diabetes</li>
<li><code>gender</code>: the patient is male(1) or female(0)</li>
<li><code>numAge</code>: patient age in years</li>
<li><code>bmi</code>: patient’s BMI</li>
<li><code>tchol</code>: patient’s total cholesterol value</li>
<li><code>sbp</code>: patient’s systolic blood pressure</li>
</ul>
<div id="split-the-data" class="section level3">
<h3>Split the Data</h3>
<p>To train and validate our model, we need separate it into separate training and testing datasets. We’ll use the <code>createDataPartition()</code> function availabe in the <code>caret</code> package. We will perform and 80/20 split on the dataset.
After splitting, we inspect the distribution of CVD patients in the training dataset and find that the <code>N</code> cases (people without CVD) are more highly represented. This can lead machine learning models to overfit and underfit to the over- and under-represented classes, respectively.</p>
<pre class="r"><code>## Partition dataset into train (80%) and test (20%)
train_idx &lt;- createDataPartition(cvd_patient$cvd, p = 0.80, list = FALSE)
train_data &lt;- cvd_patient[train_idx, ]
test_data &lt;- cvd_patient[-train_idx, ]

## Inspect class distribution
table(train_data$cvd)</code></pre>
<pre><code>## 
##      N      Y 
## 300260  39896</code></pre>
</div>
<div id="downsample-the-data" class="section level3">
<h3>Downsample the Data</h3>
<p>To mitigate this issue, we will use downsampling via the <code>downSample()</code> function available in <code>caret</code>. This function randomly removes cases from the over-represented class until the class distributions are equal. Two minor issues with this function are that it (A) creates a new column called <code>Class</code> which contains the same data in the <code>cvd</code> column and (B) sorts the data by class. So, we’ll need to remove the <code>Class</code> column and reshuffle our dataset.</p>
<pre class="r"><code>## Downsample the dataset so that classes are balanced
train_data &lt;- 
  train_data %&gt;%
  downSample(train_data$cvd) %&gt;%
  select(-Class)

## Because downSample() groups by class, reshuffle the downsampled dataset
shuffle_idx &lt;- sample(nrow(train_data))
train_data &lt;- train_data[shuffle_idx,]

## Inspect class distribution
table(train_data$cvd)</code></pre>
<pre><code>## 
##     N     Y 
## 39896 39896</code></pre>
</div>
<div id="scale-the-data" class="section level3">
<h3>Scale the Data</h3>
<p>For many machine learning models to perform adequately, the input data must be scaled into the [-1, 1] or [1, 1] range. We create a simple function, <code>scale_data</code>, that takes as input the dataframe we’d like to scale and the column indeces of that dataframe that should be kept. It then converts that dataframe into a matrix, where all variables are converted to numeric. We then apply the function <span class="math inline">\(f(x) = \frac{x - min(x)}{max(x) - min(x)}\)</span> to each column specified by <code>keep</code> and return the dataframe.</p>
<p>NOTE: In practice, we would scale the test dataset by the same values used to scale the training dataset. To keep things simple, we’re just going to scale each dataset by its own features.</p>
<pre class="r"><code>scale_data &lt;- function(data, keep) {
  data[keep] %&gt;%
    data.matrix() %&gt;%
    apply(2, function(x) (x - min(x)) / (max(x) - min(x))) %&gt;%
    return()
}

## Scale numeric data in train and test datasets into the [0, 1] range
x_train &lt;- scale_data(train_data, 1:9)
y_train &lt;- scale_data(train_data, 10)

x_test &lt;- scale_data(test_data, 1:9)
y_test &lt;- scale_data(test_data, 10)

head(x_train)</code></pre>
<pre><code>##       htn treat smoking t2d gender    numAge       bmi     tchol       sbp
## 1526    0     0       0   0      1 0.4444444 0.3809524 1.0000000 0.3873239
## 66124   1     0       0   0      0 0.6555556 0.3333333 0.0000000 0.7464789
## 55313   1     0       0   0      1 0.6555556 0.5714286 0.3444444 0.7394366
## 21535   0     0       0   0      0 0.3777778 0.2857143 0.0000000 0.3380282
## 44120   0     0       0   0      0 0.5111111 0.2857143 0.4555556 0.2676056
## 15791   0     0       0   0      0 0.4555556 0.7142857 0.1222222 0.3873239</code></pre>
<pre class="r"><code>head(y_train)</code></pre>
<pre><code>##       cvd
## 1526    0
## 66124   1
## 55313   1
## 21535   0
## 44120   1
## 15791   0</code></pre>
</div>
</div>
<div id="model-training" class="section level2">
<h2>Model Training</h2>
<div id="setup-the-model" class="section level3">
<h3>Setup the Model</h3>
<p>Now we can begin setting up our model. The command <code>Sys.setenv('KMP_DUPLICATE_LIB_OK'='T')</code> is simply here to prevent an error that can occur for Mac users with certain Intel CPUs. You might not need it, but it also won’t hurt.</p>
<p>So let’s build our simple feedforward neural network using <code>keras</code>, an API for Google’s TensorFlow. We tell R that we’d like to build a sequential model (i.e. information is passed from one layer to the next), then we add a hidden layer with 100 hidden units, followed by another hidden layer with 50 units, and an output layer with 1 unit. Because we are performing binary classification, the output node has a simgoidal activation unit. The other layers use a <a href="https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning">rectified linear unit</a> for their activation function.</p>
<p>You’ll also notice that we’ve implemented <a href="https://www.kaggle.com/pavansanagapati/dropout-regularization-deep-learning">dropout</a> between each of the layers. This is a regularization technique that reduces overfitting. We’ve set the <code>rate</code> to be 0.5 – for each batch of training data passed through to the model, each node has a 50% chance of being randomly ignored. This means that, for that specific training batch, the model must learn to make predictions without those nodes. Essentially, this prevents neighboring nodes from becoming to reliant upon one another so that they may better generalize to other datasets.</p>
<pre class="r"><code>## Avoids errors on certain Intel CPUs on MacOS
Sys.setenv(&#39;KMP_DUPLICATE_LIB_OK&#39;=&#39;T&#39;)

## Build the deep learning model
model &lt;-
  keras_model_sequential() %&gt;%
  layer_dense(100, activation = &#39;relu&#39;, input_shape = ncol(x_train)) %&gt;%
  layer_dropout(0.5) %&gt;%
  layer_dense(50, activation = &#39;relu&#39;) %&gt;%
  layer_dropout(0.5) %&gt;%
  layer_dense(1, activation = &#39;sigmoid&#39;) %&gt;%
  compile(
    loss = &quot;binary_crossentropy&quot;,
    optimizer = optimizer_sgd(lr = 1.0, nesterov = T, momentum = 0.9),
    metrics = c(&#39;acc&#39;)
  )</code></pre>
</div>
<div id="train-the-model" class="section level3">
<h3>Train the Model</h3>
<p>On to the fun part…training the model! We create an object, <code>train</code> which fits our model, <code>model</code>, using the specified parameters. Hopefully <code>x</code> and <code>y</code> are self-explanatory. <code>validation_split</code> tells the train generator that we’d like to use the <em>last</em> 20% of our dataset (this is why we had to reshuffle our dataset) as a held-out validation set so that we can monitor the training process and prevent overfitting.</p>
<pre class="r"><code>## Fit the deep learning model to the training dataset
train &lt;-
  fit(
    model,
    x = x_train,
    y = y_train,
    validation_split = 0.20,
    batch_size = 5000,
    epochs = 10
  )

## Plot the learning metrics
plot(train)</code></pre>
<p><img src="https://aaroncoyner.github.io/posts/lime_interpretation_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>We can see that, during training, the training and validation accuracy and loss metrics tracked nicely together. This suggests that the model is not overfit to the dataset. If anything, it’s slightly underfit, as suggested by the fact that the validation set accuracy is higher than the training set accuracy and the validation set loss is lower that the training set loss.</p>
</div>
</div>
<div id="model-evaluation" class="section level2">
<h2>Model Evaluation</h2>
<div id="compute-informative-model-metrics" class="section level3">
<h3>Compute Informative Model Metrics</h3>
<p>Just because we’ve shown that our model is not overfit to that particular dataset, we still need to demonstrate its generalizability to the held out test dataset. To do so, we’ll first have the model predict the probability of each subject in the test dataset as having CVD using <code>predict_proba()</code> from <code>keras</code>. We’ll then use the <code>prediciton()</code> function in <code>caret</code> to compare the probabilities to the true test dataset labels. <code>performance()</code> uses those predictions to evaluate the true and false positive rates at each probability cutoff/threshold. This will create a receiver operating characteristics curve. We’ll also evaluate the area under this curve as an overall measure of model performance.</p>
<pre class="r"><code>## Predict the probability of CVD on the test dataset
nn_predictions &lt;- predict_proba(model, x_test)

## Compare the predictions to the true class label
nn_pr &lt;- prediction(nn_predictions, y_test)

## Calculate the AUC of the model
nn_auc &lt;- performance(nn_pr, measure = &quot;auc&quot;)
nn_auc &lt;- nn_auc@y.values[[1]]

## Evaluate the sensitivity and specificity at each threshold.
## Save output into a dataframe.
nn_perf &lt;- performance(nn_pr, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)
nn_perf_df &lt;- data.frame(nn_perf@x.values, nn_perf@y.values)
colnames(nn_perf_df) &lt;- c(&quot;x_values&quot;, &quot;y_values&quot;)

## Plot the results
ggplot(nn_perf_df, aes(x = x_values, y = y_values)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, color = &quot;red&quot;) +
  annotate(
    geom = &quot;text&quot;,
    x = 0.75,
    y = 0.32,
    label = paste(&#39;AUC: &#39;, round(nn_auc, 3))
  ) +
  labs(
    title = &quot;Receiver Operating Characteristics Curve&quot;,
    subtitle = &quot;Test Dataset&quot;,
    x = &quot;1 - Specificity&quot;,
    y = &quot;Sensitivity&quot;
  ) +
  theme_light()</code></pre>
<p><img src="https://aaroncoyner.github.io/posts/lime_interpretation_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Let’s also use the <code>confusionMatrix()</code> function to evaluate model performance at the a probability threshold of 0.5. Not only does this function create a confusion matrix for us, but it also lists the model accuracy, balanced accuracy, sensitivity, specificity, and a few others.</p>
<pre class="r"><code>## Predict the class (rather than probability) of CVD on the test dataset
nn_classes &lt;- predict_classes(model, x_test)

## Create a confusion matrix
nn_conf &lt;- confusionMatrix(as.factor(nn_classes), as.factor(y_test),positive = &#39;1&#39;)
nn_conf</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction     0     1
##          0 54624  1517
##          1 20441  8457
##                                           
##                Accuracy : 0.7418          
##                  95% CI : (0.7388, 0.7447)
##     No Information Rate : 0.8827          
##     P-Value [Acc &gt; NIR] : 1               
##                                           
##                   Kappa : 0.3158          
##                                           
##  Mcnemar&#39;s Test P-Value : &lt;2e-16          
##                                           
##             Sensitivity : 0.84790         
##             Specificity : 0.72769         
##          Pos Pred Value : 0.29265         
##          Neg Pred Value : 0.97298         
##              Prevalence : 0.11729         
##          Detection Rate : 0.09945         
##    Detection Prevalence : 0.33982         
##       Balanced Accuracy : 0.78780         
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
</div>
</div>
<div id="model-interpretability" class="section level2">
<h2>Model Interpretability</h2>
<p>Great! So far, we’ve built a fairly decent model for CVD prediction. It’s far from perfect, but it’s certainly better than chance. But how is the model making its predictions?</p>
<pre class="r"><code>summary(model)</code></pre>
<pre><code>## Model: &quot;sequential&quot;
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense (Dense)                    (None, 100)                   1000        
## ___________________________________________________________________________
## dropout (Dropout)                (None, 100)                   0           
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 50)                    5050        
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 50)                    0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 1)                     51          
## ===========================================================================
## Total params: 6,101
## Trainable params: 6,101
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<p>This summary shows that, for our “simple” feedforward neural network, there are 6,101 trainable parameters (i.e. weighted connections). Practically, we cannot infer what the 1,000 connections between the input data and the first layer mean and, unfortunately, there’s no way to know what the 5,050 connections between the first and second hidden layers or the 51 connections between the second hidden layer and the output layer actually mean.</p>
<p>Without this information, how are we supposed to use this model practically? Ultimately, our goal is to <em>prevent</em> CVD, not just detect it. <strong>We need to learn what risk factors are associated with CVD.</strong></p>
<div id="investigate-global-predictors" class="section level3">
<h3>Investigate Global Predictors</h3>
<p>To begin, we’ll fist take a look at global predictors of CVD – we will perform a correlation analysis between <code>cvd</code> and the predictors in the training dataset. We’ll use the <code>correlate()</code> function from the <code>corrr</code> package to do this, as it returns a correlation dataframe rather than a matrix. We can use this dataframe to better visualize the findings with a forest plot.</p>
<pre class="r"><code>## Create correlation table to examine features that correlate GLOBALLY
corrr_analysis &lt;-
  as.data.frame(x_train) %&gt;%
  mutate(cvd = y_train) %&gt;%
  mutate_if(is.factor, as.numeric) %&gt;%
  correlate(quiet = T) %&gt;%
  focus(cvd) %&gt;%
  rename(feature = rowname) %&gt;%
  arrange(abs(cvd)) %&gt;%
  mutate(feature = as_factor(feature))

corrr_analysis</code></pre>
<pre><code>## # A tibble: 9 x 2
##   feature     cvd
##   &lt;fct&gt;     &lt;dbl&gt;
## 1 bmi      0.0657
## 2 tchol   -0.0751
## 3 t2d      0.0858
## 4 smoking  0.136 
## 5 gender   0.174 
## 6 treat    0.329 
## 7 sbp      0.433 
## 8 htn      0.436 
## 9 numAge   0.538</code></pre>
<pre class="r"><code>## Create a forest plot of the global correlation metrics
corrr_analysis %&gt;%
  ggplot(aes(x = cvd, y = fct_reorder(feature, desc(cvd)))) +
  geom_point() +
  geom_segment(
    aes(xend = 0, yend = feature),
    color = palette_light()[[2]],
    data = filter(corrr_analysis, cvd &gt; 0)
  ) +
  geom_point(
    color = palette_light()[[2]],
    data = filter(corrr_analysis, cvd &gt; 0)
  ) +
  geom_segment(
    aes(xend = 0, yend = feature),
    color = palette_light()[[1]],
    data =  filter(corrr_analysis, cvd &lt; 0)
  ) +
  geom_point(
    color = palette_light()[[1]],
    data = filter(corrr_analysis, cvd &lt; 0)
  ) +
  geom_vline(
    xintercept = 0,
    color = palette_light()[[5]],
    size = 1,
    linetype = 2
  ) +
  geom_vline(
    xintercept = -0.25,
    color = palette_light()[[5]],
    size = 1,
    linetype = 2
  ) +
  geom_vline(
    xintercept = 0.25,
    color = palette_light()[[5]],
    size = 1,
    linetype = 2
  ) +
  theme_tq() +
  labs(
    title = &#39;CVD Correlation Analysis&#39;,
    subtitle = &#39;Negative vs. Positive Correlations&#39;,
    x = &#39;CVD Risk&#39;,
    y = &#39;Feature Importance&#39;
  )</code></pre>
<p><img src="https://aaroncoyner.github.io/posts/lime_interpretation_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>This plot shows that <code>numAge</code>, <code>htn</code>, <code>sbp</code>, and <code>treat</code> are correlated with <code>cvd</code>. <strong>How are <code>htn</code>, <code>sbp</code> and <code>treat</code> related?</strong> It could be argued that <code>gender</code> and <code>smoking</code> are loosely correlated with <code>cvd</code>, too. But how do we use this information? Do we just assume older people will have CVD? Older people with hypertension? Older people with hypertension who are also receiving treatment for hypertension? This plot is great for identifying the global drivers of CVD, but we need to investigate locally if we want to learn how these predictors interact.</p>
</div>
<div id="investigate-local-predictors" class="section level3">
<h3>Investigate Local Predictors</h3>
<p>We can use Local Interpretable Model-agnostic Explanations (LIME) to achive this. Unfortunately, the <code>lime</code> package is not setup out-of-the-box to work with <code>keras</code>, so we need to make two custom functions for it to work properly:</p>
<ul>
<li><code>model_type()</code>: Used to tell lime what type of model we are dealing with. It could be classification, regression, survival, etc.</li>
<li><code>predict_model()</code>: Used to allow lime to perform predictions that its algorithm can interpret.</li>
</ul>
<pre class="r"><code>## Define a new classification model type for LIME
model_type.keras.engine.sequential.Sequential &lt;- function(x, ...) {
  return(&quot;classification&quot;)
}

## Create a prediction wrapper around predict_proba for LIME
predict_model.keras.engine.sequential.Sequential &lt;- function (x, newdata, type, ...) {
  pred &lt;- predict_proba(object = x, x = as.matrix(newdata))
  return(data.frame(Positive = pred, Negative = 1 - pred))
}</code></pre>
<p>Now, we create an explainer object and use it to explain 4 randomly-sampled observations from our test dataset. We can visualize these test cases using the <code>plot_features()</code> function.</p>
<pre class="r"><code>## Create LIME explainer object
explainer &lt;- lime(as.data.frame(x_train), model, quantile_bins = FALSE, n_bins = 2)

## Randomly sample from the training dataset
samples &lt;- sample(1:nrow(x_train), 4)

## Use explainer object to explain subset of data
explanation &lt;-
  explain(
    as.data.frame(x_test)[samples, ], ## Randomly select 4 sample cases
    explainer = explainer,
    n_labels = 1, ## Explain a single class (i.e. CVD)
    n_features = 3 ## Return the top five features critical to the case
  ) 

## Plot the features that correlate LOCALLY for the single case using a bar chart
plot_features(explanation) +
  labs(
    title = &quot;LIME: Feature Importance Visualization&quot;,
    subtitle = &quot;Hold Out (Test) Set, 4 Cases Shown&quot;
  )</code></pre>
<p><img src="https://aaroncoyner.github.io/posts/lime_interpretation_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>test_data[samples, ]</code></pre>
<pre><code>##        htn treat smoking t2d gender numAge bmi tchol sbp cvd
## 249332   Y     Y       N   N      F     65  30   242 167   Y
## 319838   N     N       N   N      F     51  21   159 139   N
## 376880   N     N       N   N      F     51  21   185 125   N
## 220150   N     N       N   N      F      4  23   197 129   N</code></pre>
<p>If we want to evaluate more than just a few observations at a time, we can use the <code>plot_explanations()</code> funciton instead. This creates a heatmap of many cases, rather than individual barplots for a select few cases.</p>
<pre class="r"><code>## Use explainer object to explain subset of data
explanation &lt;-
  explain(
    as.data.frame(x_test)[sample(1:nrow(x_train), 20), ], ## Randomly select 20 cases
    explainer = explainer,
    n_labels = 1, # Explaine a single class (i.e. CVD)
    n_features = 3 # Return the top five features critical to EACH case
  )

## Plot the features that correlate LOCALLY for ALL cases using a heatmap
plot_explanations(explanation) +
  labs(
    title = &quot;LIME Feature Importance Heatmap&quot;,
    subtitle = &quot;Hold Out (Test) Set, First 20 Cases Shown&quot;
  )</code></pre>
<p><img src="https://aaroncoyner.github.io/posts/lime_interpretation_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
</div>

      </div>

      <footer>
        


        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "aaroncoyner" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </footer>
    </article>

    
  </section>

      </div>

      <footer class="footer">
  <section class="container">
    
     © Aaron Coyner 2019. All Rights Reserved.
    
    
  </section>
</footer>

    </main>

    

  </body>

</html>
